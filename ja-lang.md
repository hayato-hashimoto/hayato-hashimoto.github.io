BERT Language models.

<ul>
  <li>BCCWJ-RoBERTa-base (⚡ Under Training)
    <dl>
      <dt>Tokenization</dt>
      <dd>Pretokenized (Unidic SUW)</dd>
      <dt>Corpus</dt>
      <dd>0.69 GB - BCCWJ (only)</dd>
      <dt>Example</dt>
      <dd> Input: '以前 取り 上げ た <mask> で ある'
        <ul><li>以前 取り 上げ た <strong>もの</strong> で ある</li>
          <li>以前 取り 上げ た <strong>の</strong> で ある
          <li>以前 取り 上げ た <strong>写真</strong> で ある</li>
          <li>以前 取り 上げ た <strong>物</strong> で ある</li>
          <li>以前 取り 上げ た <strong>商品</strong> で ある</li>
    </dl>
  </li>
 </ul>
